Hello, world!
I ❤ Python

See you later!

CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



CSDN首页
博客
专栏课程
下载
问答
社区
插件
认证
Python文件操作
 搜索

会员中心 
收藏
动态
消息
创作
Python 读取大文件

一去丶二三里 2017-10-11 18:18:49  28318  收藏 18
分类专栏： Python 快速入门 文章标签： Python Python读大文件 Python文件读写 Python文件操作 Python文件管理
版权

Python 快速入门
专栏收录该内容
57 篇文章147 订阅
订阅专栏
简述
在处理大数据时，有可能会碰到好几个 G 大小的文件。如果通过一些工具（例如：NotePad++）打开它，会发生错误，无法读取任何内容。

那么，在 Python 中，如何快速地读取这些大文件呢？

| 版权声明：一去、二三里，未经博主允许不得转载。

一般的读取
读取文件，最常见的方式是：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f.readlines():
        do_something(line)
1
2
3
但是，当完成这一操作时，readlines() 方法（read() 也一样）会将整个文件加载到内存中。在文件较大时，往往会引发 MemoryError（内存溢出）。

那么，如何避免这个问题？

使用 fileinput 模块
稍微好点儿的方式是使用 fileinput 模块：

import fileinput

for line in fileinput.input(['filename']):
    do_something(line)
1
2
3
4
调用 fileinput.input() 会按照顺序读取行，但是在读取之后不会将它们保留在内存中。

逐行读取
除此之外，也可使用 while() 循环和 readline() 来逐行读取：

with open('filename', 'r', encoding = 'utf-8') as f:
    while True:
        line = f.readline()  # 逐行读取
        if not line:  # 到 EOF，返回空字符串，则终止循环
            break
        do_something(line)
1
2
3
4
5
6
指定每次读取的长度
有时，可能希望对每次读取的内容进行更细粒度的控制。

在这种情况下，可以使用 iter 和 yield：

def read_in_chunks(file_obj, chunk_size = 2048):
    """
    逐件读取文件
    默认块大小：2KB
    """
    while True:
        data = file_obj.read(chunk_size)  # 每次读取指定的长度
        if not data:
            break
        yield data

with open('filename', 'r', encoding = 'utf-8') as f:
    for chuck in read_in_chunks(f):
        do_something(chunk)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
自动管理
这才是 Pythonci 最完美的方式，既高效又快速：

with open('filename', 'r', encoding = 'utf-8') as f:
    for line in f:
        do_something(line)
1
2
3
with 语句句柄负责打开和关闭文件（包括在内部块中引发异常时），for line in f 将文件对象 f 视为一个可迭代的数据类型，会自动使用 IO 缓存和内存管理，这样就不必担心大文件了。

更多参考
How to read large file, line by line in python

一去丶二三里
关注

13

4

18
一键三连

专栏目录
python读取大文件越来越慢的原因与解决
09-18
主要给大家介绍了关于python读取大文件越来越慢的原因与解决方法，文中通过示例代码介绍的非常详细，对大家学习或者使用Python具有一定的参考学习价值，需要的朋友们下面来一起学习学习吧
Python chunk读取超大文件
qfikh的博客
 2794
1. 普通文件读取方式： import pandas as pd df = pd.read_csv('./chunk_test.csv') # 默认 iterator=False,chunksize=None print(df.shape,'\n',df.head()) 一般使用read_csv的时候，iterator是设定为False的，这个时候read_csv会把整个文件的数据读取到Da...

优质评论可以帮助作者获得更高权重
表情包
Aifore
Aifore:直接遍历一个文件对象？学习了3 年前回复
点赞
2
u011012932
一去丶二三里:这是一个示例而已，改成你自己的业务方法！3 年前回复
点赞
1
qq_40941051
qq_40941051:do_something(line)这个方法不给？？？3 年前回复
点赞
用Python读取大文件(上)_hackstoic的博客
9-20
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成: f = open(filename,'r') f.read() 这种方法读取小文件,即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取,即...
python快速读取非常大的文件_coordinate的博客
8-12
python快速读取非常大的文件 读取大文件是我们平时经常会遇到的问题,我这里给出两个比较好的解决方案。第一种 withopen("test.txt")asf:forlineinf:#do something with data 这种做法非常的简单。这个代码在打开文件的过程中,不会一次...
Python从数据库读取大量数据批量写入文件的方法
09-19
今天小编就为大家分享一篇Python从数据库读取大量数据批量写入文件的方法，具有很好的参考价值，希望对大家有所帮助。一起跟随小编过来看看吧
强悍的 Python —— 读取大文件
热门推荐
https://space.bilibili.com/59807853
 2万+
Python 环境下文件的读取问题，请参见拙文 Python 基础 —— 文件 这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb') f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执
Python读取大文件方法对比_菇星獨行
7-18
Python读取大文件方法对比 参考:Lazy Method for Reading Big File in Python? 首先可以确定的是不能用read()与readlines()函数 这两个函数均将数据全部读入内存,会造成内存不足的情况。
使用Python读取大文件_笔墨留年。
8-19
今天有个朋友问了我一个问题,如何使用Python读取大文件?觉得这个问题挺有意思的,就记录下来。 大部分时间我们处理小文件的时候(1g以内?),可以直接用f.read()或readlines()直接把全部内容读取到内存里面来。但当文件非常大,比如10g,10...
python生成器实现大文件读取
MoveChx的博客
 1787
python生成器实现大文件读取 python生成器实现大文件读取 你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。 新的改变 我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客： 全...
python读取超大文件 Python读取大文件(GB)
从零开始学习python --zeropython
 4566
最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 我们谈到“文本处理”时，我们通常是指处理的内容。Python 将文本文件的内容读入可以操作的字符串变量非常容易。文件对象提供了三个“读”方法...
python 读取大文件
鬼小七的博客
 9267
python 读取大文件 python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟re...
python读取大文件-强悍的Python读取大文件的解决方案
weixin_37988176的博客
 109
Python 环境下文件的读取问题，请参见拙文 Python基础之文件读取的讲解这是一道著名的 Python 面试题，考察的问题是，Python 读取大文件和一般规模的文件时的区别，也即哪些接口不适合读取大文件。1. read() 接口的问题f = open(filename, 'rb')f.read()我们来读取 1 个 nginx 的日至文件，规模为 3Gb 大小。read() 方法执行的操作...
Python快速读取超大文件
夜空下的凝视
 8484
方法一： 测试文件共6862646行，79.3M大小，耗时6.7秒。 缺点：每一行数据内容不能大于内存大小(一般不会)，否则就会造成MemoryError。 import time print("开始处理...") start = time.time() file = r'e:\Python\mypy\搜狗词库\sogou_jianhua_new.txt' with open(file, 'rb...
Python读取大文件(GB)
scdxmoe的专栏
 7066
http://chenqx.github.io/2014/10/29/Python-fastest-way-to-read-a-large-file/ 最近处理文本文档时（文件约2GB大小），出现memoryError错误和文件读取太慢的问题，后来找到了两种比较快Large File Reading 的方法，本文将介绍这两种读取方法。 Preliminary 　　我们谈到
Python 读取大文件 最后几行
 529
方法一： # -*- coding: utf-8 -*- import sys import os import string RCV_LOG = r"d:\c.txt" def get_last_n_lines(logfile, n): n = string.atoi(n) blk_size_max = 4096 n_lines =...
读取大文件CSV
wld的博客
 5181
    read_csv中有个参数chunksize，通过指定一个chunksize分块大小来读取文件，返回的是一个可迭代的对象TextFileReader，IO Tools 举例如下：pd.readcsv 的chunksizeIn [138]: reader = pd.read_table('tmp.sv', sep='|', chunksize=4)In [139]: readerOut[13...
Python 优雅的读取大内存文件
最新发布
weixin_39386145的博客
 465
背景： 使用 python 读取一个相对较大内存的文件，比如电脑运行内存只有1G，但是读取的文件大小为 2G,这个时候怎么处理呢？有人说直接用 open函数不就好了吗？但是如果你直接使用的话，会把文件一次性读取存在内存中，这个时候就会出现 OOM（内存溢出）即MemoryError，那么我们应该怎么处理呢？ 我们先来看看python读取文件后，获取文件内容的几种方法； 方法 作用 f.read() 读取文件中所有内容 f.readline() 读取第一行的内容 f.readlines
python快速读取非常大的文件
第一天
 7271
读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种 with open("test.txt") as f: for line in f: #do something with data 这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于buffer机制。 当然我们也可以自己去实现一个buff...
python读取大文件
weixin_33918114的博客
 135
最近在学习python的过程中接触到了python对文件的读取。python读取文件一般情况是利用open()函数以及read()函数来完成： f = open(filename,'r') f.read() 这种方法读取小文件，即读取远远大小小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大于内存的大小，这么处理就有问题了，会造成MemoryError...
Python3读取大文件的方法
Jock2018的博客
 8803
Python3读取大文件的方法1. 方法一：利用yield生成器2. 方法二：利用open()自带方法生成迭代对象，这个是一行一行的读取3. 二者的比较 1. 方法一：利用yield生成器 def readPart(filePath, size=1024, encoding="utf-8"): with open(filePath,"r",encoding=encoding) as f: ...
强悍的Python读取大文件的解决方案
09-19
今天小编就为大家分享一篇关于强悍的Python读取大文件的解决方案，小编觉得内容挺不错的，现在分享给大家，具有很好的参考价值，需要的朋友一起跟随小编来看看吧
pypacker：Python最快最简单的数据包处理库-源码
02-04
请注意：由于移至GitLab，此存储库已停滞。 访问以获取最新版本。 一般信息 这就是Pypacker：Python最快，最简单的数据包处理库。 它使您可以通过定义所有标头数据的各个方面来手动创建数据包，通过解析原始数据包字节，在不同层上发送/接收数据包以及截取数据包来解剖数据包。 Pypacker可以做什么 创建提供特定值的数据包或采用默认值： from pypacker . layer3 . ip import IP from pypacker . layer3 . icmp import ICMP ip = IP ( src_s = "127.0.0.1" , dst_s = "192
Deepin应用商店打开后不显示（deepin 15.11）
01-20
打开应用商店后，界面是这样，查验过，网络正常，卸载重装仍旧无法解决。 由于无法打开应用商店基本都是软件源的问题。 1.deepin的官网去查看应用的软件源地址。 会发现这样一句话： 下面显示的是深度操作系统15默认软件源： deb [by-hash=force] http://packages.deepin.com/deepin stable main contrib non-free #deb-src http://packages.deepin.com/deepin stable main contrib non-free 查看本地系统的软件源文件，cat /etc/apt/source.
python快速读取大数据
weixin_30897079的博客
 1035
rd = pd.read_csv(path7 + u'0501-0914.csv',encoding = 'gbk',iterator = True) loop =True dflst = [] i = 0 while loop: try: i+=1 df0 = rd.get_chunk(50000) dflst.appe...
©️2020 CSDN 皮肤主题: 代码科技 设计师:Amelia_0503 沉浸阅读: CSDN浏览器助手 返回首页
关于我们
招贤纳士
广告服务
开发助手

400-660-0108

kefu@csdn.net

在线客服
工作时间 8:30-22:00
公安备案号11010502030143
京ICP备19004658号
京网文〔2020〕1039-165号
经营性网站备案信息
北京互联网违法和不良信息举报中心
网络110报警服务
中国互联网举报中心
家长监护
Chrome商店下载
©1999-2021北京创新乐知网络技术有限公司
版权与免责声明
版权申诉
出版物许可证
营业执照
目录
简述
一般的读取
使用 fileinput 模块
逐行读取
指定每次读取的长度
自动管理
更多参考



